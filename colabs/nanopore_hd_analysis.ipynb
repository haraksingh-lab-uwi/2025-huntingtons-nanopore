{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Molecular diagnosis of Huntington’s Disease in Trinidadian families via Triplet Repeat Primed PCR, Fragment analysis, and Nanopore sequencing\n",
        "\n",
        "Shavana Nicole Rajkumar<sup>1</sup>, Chris Gyan<sup>2</sup>, Damion Basdeo<sup>2</sup>, Nemal Gokool<sup>2</sup>, Arianne Brown Jordan<sup>3</sup>, Soren Nicholls<sup>3</sup>, Vijay Pradeep<sup>4</sup>, and Rajini Rani Haraksingh<sup>1</sup>\n",
        "\n",
        "<sup>1</sup>Department of Life Sciences, Faculty of Science and Technology, The University of the West Indies, St. Augustine, Trinidad and Tobago<br>\n",
        "<sup>2</sup>Sangre Grande Hospital, Eastern Regional Health Authority, Trinidad & Tobago<br>\n",
        "<sup>3</sup>Department of Preclinical Sciences, Faculty of Medical Sciences, The University of the West Indies, St. Augustine, Trinidad and Tobago<br>\n",
        "<sup>4</sup>Virtana TT Ltd, St. Augustine, Trinidad and Tobago\n",
        "\n",
        "## Abstract\n",
        "**Background:** Huntington’s Disease (HD) is a neurodegenerative disorder caused by a CAG expansion in the Huntingtin (HTT) gene. Due to its non-specific and variable phenotype, diagnosis requires clinical assessments and genetic testing. In the Caribbean, the genetic etiology of HD is underexplored due to the unavailability of genetic testing.\n",
        "\n",
        "**Objectives:** We investigated whether 32 participants from 4 multigenerational families from Trinidad and Tobago (T&T) presenting with Huntington-like symptoms carried HTT CAG expansions, and whether progressive expansion was related to decreasing age of onset with each generation.\n",
        "\n",
        "**Results:** All symptomatic participants carried HTT CAG expansions (42-57 CAGs), confirming HD. Among participants aged 20-65 years (n=24), clinical and genetic diagnoses were 100% concordant for 22 participants (13 symptomatic with 42-57 CAGs, and 9 asymptomatic with 13-27 CAGs). Two asymptomatic participants aged 22 and 43 years carried 46-47 and 37-39 CAGs, respectively. Among 8 participants &lt;18 years, one symptomatic 16 year old carried 49-50 CAGs, and 7 are currently asymptomatic (3 with 50-52 CAGs, and 4 with 14-17 CAGs). In all families, progressive expansion and decreasing age of onset was observed in each successive generation. Methods were highly correlated (r2 = 0.998).\n",
        "\n",
        "**Conclusions:** We demonstrated the novel application of nanopore sequencing with a custom bioinformatics workflow to accurately size HTT CAG repeats. This is the first genetic report of HD in T&T, among limited records in the Caribbean.\n",
        "\n",
        "Link to Publication: *(TODO)*\n"
      ],
      "metadata": {
        "id": "UPhS0kjH2bpT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD-8L-tJdv2F"
      },
      "source": [
        "# Running the Google Colab Informatics Pipeline:\n",
        "* Load this notebook into Google Colab ([or follow this link](https://colab.research.google.com/drive/1MIvhA1dgq093tx29I-hkY7WooaI4Mn25))\n",
        "* Find files barcode01.fastq.gz to barcode23.fastq.gz on your local computer.\n",
        "* In the toolpane on the left side, find the 'files' option\n",
        "* Create a folder in the colab called 'input'\n",
        "* Upload all *.fastq.gz files into the input folder\n",
        "* Run the script (Go to runtime->run all)\n",
        "* Text output is printed inline. Graphs are printed inline, and also saved as individual files within the colab instance (available via the file browser in the lefthand toolbar).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies\n",
        " * [fastq](https://github.com/not-a-feature/fastq) - Used to parse the fastq input files\n",
        " * [swalign](https://github.com/mbreese/swalign) - Used for doing Smith-Waterman style local alignment\n",
        " * [miniFasta](https://github.com/not-a-feature/miniFASTA) - Used by fastq"
      ],
      "metadata": {
        "id": "6VRcrmF2s1w-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0JMytjJ8xgJ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install fastq==2.0.2 swalign==0.3.7 miniFasta==3.0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load cached results, if available\n",
        "\n",
        "The Smith-Waterman alignment is the most time consuming step in the overall pipeline (Approx 5-10 minutes per sample, depending on the number of reads). As such, this script optionally saves the Smith-Waterman results into a python 'pickle' file, so that they can be easily reloaded back into memory.\n",
        "\n",
        "To use the previously cached results, they must be uploaded to session storage at `preprocessed/barcodes-pkl-0-24.tgz`, via the colab's file manager, available in the toolbar on the left side of the editor.\n",
        "\n",
        "You can also download previously cached results, generated by the authors, from the following Google Drive link: [2024-02-22-barcodes-pkl-0-24.tgz](https://drive.google.com/file/d/1DlclntCt__4KTni4jrezbohci8bg3o1h/view?usp=drive_link)"
      ],
      "metadata": {
        "id": "VJ3VwGMQt0K_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if os.path.exists('preprocessed/2024-02-22-barcodes-pkl-0-24.tgz'):\n",
        "  !tar xzf preprocessed/2024-02-22-barcodes-pkl-0-24.tgz -C preprocessed/\n"
      ],
      "metadata": {
        "id": "Ws1kvw16hlky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choose what samples to process\n",
        "\n",
        "While testing the pipeline or experimenting with alternative implementations, it can be helpful to adjust this section to only 1 or 2 samples to save time."
      ],
      "metadata": {
        "id": "HPLzbFCvuWDe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTK5_9_EJreD"
      },
      "outputs": [],
      "source": [
        "sample_names = ['barcode%02i' % x for x in range(1,25)]\n",
        "print(sample_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run alignment on all samples\n",
        "\n",
        "For each sample, the fastq reads are loaded and parsed. Forward and reverse start and end primers are aligned to each read using two methods:\n",
        " * **Regular Expressions** - This approach only works when there is a perfect match. This approach is only used for debugging purposes and to help sanity-check that Smith-Waterman alignment is operating as expected.\n",
        " * **Smith-Waterman Alignment** - This allows for imperfect matches. Matches are scored as +1 for every match, and -1 for every mismatch.\n",
        "\n",
        " The matching results for each sample are stored in the `Preprocessed` object, which can be used for further analysis.\n",
        "\n",
        " Each `Preprocessed` object is also stored to disk as a [python pickle](https://docs.python.org/3/library/pickle.html) file (`.pkl`) to serve as a cache.  If the pickle file is found on disk, then these results are loaded instead of rerunning the alignment.\n",
        "\n",
        " Reads should be uploaded to `input/barcode[i].fastq.gz` for i=00,01...24"
      ],
      "metadata": {
        "id": "ZO8RdfKhKcqD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUJOg3HssBnb"
      },
      "outputs": [],
      "source": [
        "import fastq as fq\n",
        "import swalign\n",
        "import os\n",
        "import types\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "class PreprocessedSample:\n",
        "    def __init__(self, sample_name):\n",
        "        self.sample_name = sample_name\n",
        "        self.raw_reads = None\n",
        "\n",
        "    def load_file(self, filename=None, max_lines=None):\n",
        "        if filename:\n",
        "            self.filename = filename\n",
        "        else:\n",
        "            self.filename = os.path.join('input', self.sample_name + \".fastq.gz\")\n",
        "        print(\"-------\")\n",
        "        print(f\"Processing File '{self.filename}'\")\n",
        "        raw_reads_gen = fq.read(self.filename)\n",
        "        self.raw_reads = list(raw_reads_gen)  # Convert to list for simplicity\n",
        "        print(f\"Loaded {len(self.raw_reads)} reads\")\n",
        "        if max_lines and len(self.raw_reads) > max_lines:\n",
        "            print(f\"Trimming reads down to {max_lines}\")\n",
        "            self.raw_reads = self.raw_reads[:max_lines]\n",
        "\n",
        "    def preprocess_re(self, cur_primers):\n",
        "        print(f\"{self.sample_name} - Performing RegEx Matching:\")\n",
        "        self.re = types.SimpleNamespace()\n",
        "        self.re.raw = {}\n",
        "        # Loop over each primer pair, and rerun search\n",
        "        for primer_name, primer_vals in cur_primers.items():\n",
        "            assert len(primer_vals) == 2\n",
        "            self.re.raw[primer_name] = [None, None]\n",
        "            for i, primer_val in enumerate(primer_vals):\n",
        "                print(f\"  Processing {primer_name} - {i}\")\n",
        "                raw_re_match = [re.search(primer_val, x.body) for x in self.raw_reads]\n",
        "                self.re.raw[primer_name][i] = [ x.span() if x else None for x in raw_re_match]\n",
        "                count = len([x for x in self.re.raw[primer_name][i] if x is not None])\n",
        "                print(f\"    Got {count} matches\")\n",
        "\n",
        "    def preprocess_sw(self, cur_primers):\n",
        "        print(f\"{self.sample_name} - Performing Smith-Waterman Matching:\")\n",
        "        self.sw = types.SimpleNamespace()\n",
        "        sw = swalign.LocalAlignment(scoring_matrix=swalign.IdentityScoringMatrix(match=1, mismatch=-1))\n",
        "        # Loop over each primer pair, and rerun search\n",
        "        self.sw.raw = {}\n",
        "        for primer_name, primer_vals in cur_primers.items():\n",
        "            assert len(primer_vals) == 2\n",
        "            self.sw.raw[primer_name] = [None, None]\n",
        "            for i, primer_val in enumerate(primer_vals):\n",
        "                print(f\"  Processing {primer_name} - {i}\")\n",
        "                self.sw.raw[primer_name][i] = [ sw.align(primer_val, x.body) for x in self.raw_reads ]\n",
        "                count_0_errors = len([x for x in self.sw.raw[primer_name][i] if x.score == len(primer_val)])\n",
        "                print(f\"    Got {count_0_errors} matches with 0 errors\")\n",
        "\n",
        "primers = dict()\n",
        "primers['f53'] = ('ATGAAGGCCTTCGAGTCCCTCAAGTCC','CAGCAGCAGCAGCAGCAACAGCCGCCACCG')\n",
        "primers['r53'] = ('CGGTGGCGGCTGTTGCTGCTGCTGCTGCTG','GGACTTGAGGGACTCGAAGGCCTTCAT')\n",
        "\n",
        "pickle_dir = 'preprocessed'\n",
        "if not os.path.exists(pickle_dir):\n",
        "    os.makedirs(pickle_dir)\n",
        "\n",
        "preprocessed_samples = dict()\n",
        "for sample_name in sample_names:\n",
        "    pickle_file = os.path.join(pickle_dir, sample_name + '.pkl')\n",
        "    if os.path.isfile(pickle_file):\n",
        "        print(f'Loading {sample_name} from pkl')\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            sample = pickle.load(f)\n",
        "    else:\n",
        "        print(f'Loading {sample_name} from raw reads')\n",
        "        sample = PreprocessedSample(sample_name)\n",
        "        sample.load_file(max_lines=None)\n",
        "        sample.preprocess_re(primers)\n",
        "        sample.preprocess_sw(primers)\n",
        "\n",
        "        # Save to pickle\n",
        "        print(f'Saving {sample_name} to pickle')\n",
        "        with open(pickle_file, 'wb') as f:\n",
        "            pickle.dump(sample, f)\n",
        "    print(f\"  {len(sample.raw_reads)} Reads Found\")\n",
        "    preprocessed_samples[sample_name] = sample\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute Read Lengths\n",
        "\n",
        "The read length (i.e. the # of nucleotides between the start and end primer) for each is calculated, based on various criteria.  A read is considered valid if both of the following criteria are met:\n",
        "* The start primer and end primer both have an acceptable alignment matching score.\n",
        "* The start primer appears before the end primer in the read\n",
        "\n",
        "This calculation is repeated for regular expression alignment, and then Smith-Waterman, with the acceptable matching score being incrementally loosened from 0 mismatches all the way to 10 mismatches.\n",
        "\n",
        "Additional diagnostic information is printed for both forward and reverse primers to aid in debugging:\n",
        "* *Start* - The number of reads where the start primer was found\n",
        "* *End* - The number of reads where the end primer was found\n",
        "* *Both* - The number of reads where both the start & end primer were found\n",
        "* *Seq* - The number of reads where the start & end primer were found sequentially (i.e. start comes before end).  These are the only reads where a read length was successfully computed.\n",
        "\n",
        "Note: As the number of allowed Smith-Waterman matches increases, it's possible to get an increase in spurious matches. In some extreme cases, a single read will have start and end primer matches for both the forward and reverse primers, which is biochemically nonsensical. This is flagged as an error and this specific read is ignored."
      ],
      "metadata": {
        "id": "7iJ0hcmhR5y0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXrXgIcZzSgM"
      },
      "outputs": [],
      "source": [
        "class GenericMatches:\n",
        "    def __init__(self, sample_name, match_method, generic_matches):\n",
        "        self.sample_name     = sample_name\n",
        "        self.match_method    = match_method\n",
        "        self.generic_matches = generic_matches\n",
        "\n",
        "    def compute_lengths(self):\n",
        "        def match_length(start, end):\n",
        "            if (start is None or end is None):\n",
        "                return -1\n",
        "            match_len = end[0] - start[1] - 1\n",
        "            if match_len < 0:\n",
        "                return -2\n",
        "            return match_len\n",
        "\n",
        "        self.lengths_per_primer = dict()\n",
        "        total_reads = len(list(self.generic_matches.values())[0][0])\n",
        "        print(f\"{self.sample_name} - Post-Processing '{self.match_method}' Matches.  {total_reads} reads\")\n",
        "        for primer_name, match_lists in self.generic_matches.items():\n",
        "            lengths = [ match_length(start, end) for start, end in zip(match_lists[0], match_lists[1]) ]\n",
        "            num_start = sum( [1 for x in match_lists[0] if x] )\n",
        "            num_end   = sum( [1 for x in match_lists[1] if x] )\n",
        "            num_both  = sum( [1 for x,y in zip(match_lists[0], match_lists[1]) if x and y])\n",
        "            num_seq   = sum( [1 for x in lengths if x >= 0])\n",
        "            print(f\"  {primer_name}:  Start:{num_start:5}  End:{num_end:5}  Both:{num_both:5}  Seq:{num_seq:5}\")\n",
        "            self.lengths_per_primer[primer_name] = lengths\n",
        "        # Compute Combined Length\n",
        "        self.lengths = []\n",
        "        for i,length_tuple in enumerate(zip(*self.lengths_per_primer.values())):\n",
        "            valid_count = sum( (1 for x in length_tuple if x >= 0) )\n",
        "            if valid_count == 0:\n",
        "                pass\n",
        "            elif valid_count == 1:\n",
        "                self.lengths.append(next((x for x in length_tuple if x >= 0)) )\n",
        "            else:\n",
        "                print(f\"Error: Detected multiple primer matches ({self.sample_name}-{i})\")\n",
        "        print(f\"  Combined: {len(self.lengths)}\")\n",
        "\n",
        "def build_generic_re_matches(preprocessed_sample):\n",
        "    generic_matches = dict()\n",
        "    for primer_name, matches in preprocessed_sample.re.raw.items():\n",
        "        generic_matches[primer_name] = [ matches[0], matches[1] ]\n",
        "    return GenericMatches(preprocessed_sample.sample_name, 're', generic_matches)\n",
        "\n",
        "def build_generic_sw_matches(preprocessed_sample, max_errors):\n",
        "    generic_matches = dict()\n",
        "    for primer_name, matches in preprocessed_sample.sw.raw.items():\n",
        "        start_matches = [ (x.q_pos, x.q_end) if (x.score >= len(x.ref) - max_errors) else None for x in matches[0] ]\n",
        "        end_matches   = [ (x.q_pos, x.q_end) if (x.score >= len(x.ref) - max_errors) else None for x in matches[1] ]\n",
        "        generic_matches[primer_name] = [ start_matches, end_matches ]\n",
        "    return GenericMatches(preprocessed_sample.sample_name, f\"sw-{max_errors}\", generic_matches)\n",
        "\n",
        "post_samples = dict()\n",
        "for sample_name, processed_sample in preprocessed_samples.items():\n",
        "    post_samples[sample_name] = types.SimpleNamespace()\n",
        "\n",
        "    post_samples[sample_name].generic_re = build_generic_re_matches(processed_sample)\n",
        "    post_samples[sample_name].generic_re.compute_lengths()\n",
        "\n",
        "    num_sw = 11\n",
        "    post_samples[sample_name].generic_sw = [None] * num_sw\n",
        "    for i in range(num_sw):\n",
        "        post_samples[sample_name].generic_sw[i] = build_generic_sw_matches(processed_sample, i)\n",
        "        post_samples[sample_name].generic_sw[i].compute_lengths()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate summary text file\n",
        "This section generates a `.csv` file with a summary of the matching results.\n",
        "Below are the headers for the output:\n",
        "* `name` - Sample Name\n",
        "* `total_reads` - The total # of reads loaded from the sample's fastq file\n",
        "* `re` - The number of reads with successful primer matches, using regular expression matching\n",
        "* `sw[i]` i = 0...10 - The number of reads with successful primer matches, using Smith-Waterman alignment, allowing up to [i] mismatches per primer."
      ],
      "metadata": {
        "id": "Tcm6EHY0Wa4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a CSV file with the specified columns\n",
        "import csv\n",
        "\n",
        "with open('num_matches_sw.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['name', 'total_reads', 're'] + [f'sw{i}' for i in range(num_sw)])\n",
        "\n",
        "    # Iterate over the post_samples dictionary and write the data to the CSV file\n",
        "    for sample_name, post_sample in post_samples.items():\n",
        "        writer.writerow([\n",
        "            sample_name,\n",
        "            len(list(post_sample.generic_re.generic_matches.values())[0][0]),\n",
        "            len(post_sample.generic_re.lengths),\n",
        "        ] + [len(post_sample.generic_sw[i].lengths) for i in range(num_sw)])\n"
      ],
      "metadata": {
        "id": "xI7Zn9M6IP6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize how many reads were successful\n",
        "This section visualizes the results from the previous csv file. This helps in building intuition around whether or not allowing more mismatches in the Smith-Waterman matching actually helps us recover for reads."
      ],
      "metadata": {
        "id": "rgbBJrQ0XrqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker\n",
        "\n",
        "# Create a figure with a single subplot\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Iterate over the post_samples dictionary and plot the data\n",
        "for i, (sample_name, post_sample) in enumerate(post_samples.items()):\n",
        "    total_reads = len(list(post_sample.generic_re.generic_matches.values())[0][0])\n",
        "    success_rate = [len(post_sample.generic_sw[i].lengths) / total_reads for i in range(num_sw)]\n",
        "    ax.plot(range(num_sw), success_rate, label=sample_name)\n",
        "\n",
        "# Add a legend and grid\n",
        "ax.grid(True)\n",
        "\n",
        "ax.set_ylim(0, 1)\n",
        "ax.yaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(1))\n",
        "\n",
        "# Set the x and y axis labels\n",
        "ax.set_xlabel('# Allowed Mismatches')\n",
        "ax.set_ylabel('% of Successful Reads')\n",
        "\n",
        "# Add a title to the plot\n",
        "ax.set_title('Success Rate vs Allowed Mismatches')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "phkqoF-fNvmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Histograms of Match Lengths\n",
        "For each alignment approach for each sample, we generate a histogram of read lengths.  These are all saved to disk and aggregated into the file `output/output.tgz`, which can be downloaded using Colab's file browser in the sidebar.\n",
        "\n",
        "Additional Notes:\n",
        "* The match length is manually increased by 15 nucleotides, since the a portion of primers themselves also contain nucleotides of interest.\n",
        "* Match lengths are capped to 300. For example, a read that is 425 long is represented as 300 on the histogram. This helps to maintain consistency across the graphs and avoid generating confusingly wide graphs."
      ],
      "metadata": {
        "id": "7K9E-fvLbu14"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sxy857EUebQJ"
      },
      "outputs": [],
      "source": [
        "# Make an output folder for the graphs\n",
        "try:\n",
        "  os.mkdir('output')\n",
        "except OSError as error:\n",
        "  if error.errno != 17:\n",
        "    raise\n",
        "  print(\"Folder already exists.  Ignoring.\")\n",
        "  pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHea1pvwuKlv"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import collections\n",
        "\n",
        "max_length = max([max(x.generic_re.lengths +\n",
        "                  [max(y.lengths) for y in x.generic_sw])\n",
        "                  for x in post_samples.values()])\n",
        "print(f\"Max Match Length is {max_length}\")\n",
        "max_length = 300\n",
        "print(f\"Hardcoding Max Match Length to {max_length}\")\n",
        "\n",
        "for sample_name, sample in post_samples.items():\n",
        "    for generic_x in [sample.generic_re] + sample.generic_sw:\n",
        "        plt.figure(figsize=(15,10))\n",
        "        adj_lengths = [min(x + 15, max_length-1) for x in generic_x.lengths]\n",
        "        plt.hist(adj_lengths, range(max_length), edgecolor='black')\n",
        "        plt.xlim(0, max_length)\n",
        "        plt.title(f'Histogram of Final Match Lengths for {sample_name}-{generic_x.match_method}')\n",
        "        plt.xlabel('Final Match Length')\n",
        "        plt.ylabel('Frequency')\n",
        "        ax = plt.gca()\n",
        "        ax.grid(which='both')\n",
        "\n",
        "        tick_locations = range(0, max_length + 1, 20)\n",
        "        tick_labels = [str(i) for i in tick_locations]\n",
        "        plt.xticks(tick_locations, tick_labels)\n",
        "\n",
        "        ax.minorticks_on()\n",
        "        ax.grid(which='minor', color='gray', linestyle=':', linewidth=0.5)\n",
        "        ax.set_xticks(range(0, max_length, 5), minor=True)\n",
        "\n",
        "        counter = collections.Counter(adj_lengths)\n",
        "        most_common = counter.most_common(6)\n",
        "        text = '\\n'.join(['(%3i,%3i)' % (value, count) for value, count in most_common])\n",
        "        text = \"Peaks:   \\n\" + text\n",
        "        plt.text(0.95, 0.95, text, transform=plt.gca().transAxes, fontsize=8,\n",
        "                verticalalignment='top', horizontalalignment='right',\n",
        "                fontdict={'family': 'monospace'})\n",
        "\n",
        "        plt.savefig(f'output/{sample_name}-{generic_x.match_method}.png')\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gSCMbdHgiFh"
      },
      "outputs": [],
      "source": [
        "# Generate a tarball of all the graphs, to simply downloading the results from the colab\n",
        "!tar -cvzf output.tgz output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example Introspection & Debugging\n",
        "\n",
        "Since intermediate state is being stored through the entire processing pipeline, it's straightforward to introspect into any specific alignment or read.\n",
        "\n",
        "In this specific case (FastQ Read #2013 from barcode01), the raw regular expression alignment & Smith-Waterman alignment matching results are being displayed from multiple stages of processing.  In this specific example, the forward starting primer aligns to two different locations, depending on whether regular expression alignment or Smith-Waterman 0-mismatch alingment is used.  In theory both algorithms should return the same alignment. However, upon closer inspection, the the start primer actually appears twice in the read, explaining the irregularity."
      ],
      "metadata": {
        "id": "xC6wUtk0Yw1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(post_samples['barcode01'].generic_re.generic_matches['f53'][0][2013])\n",
        "print(post_samples['barcode01'].generic_sw[0].generic_matches['f53'][0][2013])\n",
        "print(\"---\")\n",
        "preprocessed_samples['barcode01'].sw.raw['f53'][0][2013].dump()\n",
        "print(\"---\")\n",
        "print(preprocessed_samples['barcode01'].re.raw['f53'][0][2013])\n",
        "print(preprocessed_samples['barcode01'].raw_reads[2013].body[69:96])\n",
        "print(preprocessed_samples['barcode01'].raw_reads[2013].body[172:199])\n"
      ],
      "metadata": {
        "id": "_dfP8o2fnL8J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}